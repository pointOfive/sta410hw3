{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3aa0add",
   "metadata": {},
   "source": [
    "# STA410 Programming Portfolio Assignment 3 (25 points)\n",
    "\n",
    "Welcome.\n",
    "\n",
    "## Rules\n",
    "\n",
    "0. Point awards for assigning the correct values into variables are indicated along with each required variable assignment name.\n",
    "\n",
    "1. **Do not delete or replace cells**: this erases cell ids upon which automated scoring is based.\n",
    "    - Cell ids are supported by [notebook format 4.5](https://github.com/jupyterlab/jupyterlab/issues/9729) or greater, and [jupyterlab](https://jupyter.org/install) version\n",
    "[3.0.13 or greater](https://github.com/jupyterlab/jupyterlab/releases/tag/v3.0.13). If the environment you work in does not support cell ids you will not get any credit for your submitted homework.  [UofT JupyterHub](https://jupyter.utoronto.ca) and [Google Colab](https://colab.research.google.com) support cell ids.\n",
    "      > You may check if cell ids are present or changing at each save with `! grep '\"id\":' <path/to/notebook>.ipynb`\n",
    "\n",
    "    - *You may add cells for scratch work*, but if required answers are not submitted through the provided cells where the answers are requested your answers will not be graded.\n",
    "    - *If you accidentally delete a required cell*, try \"Edit > Undo Delete Cells\" in the notebook editor; otherwise, redownload the notebook (so it has the correct required cells ids) and repopulate it with your answers (assuming you don't overwrite them).\n",
    "    \n",
    "    \n",
    "2. **No cells may have any runtime errors**: this causes subsequent tests to fail and you will not get credit for tests which fail because of previous runtime errors.\n",
    "    - The `try`-`except` block syntax \"catches\" runtime errors and transforms them into `exceptions` which are no longer runtime errors.  These `exceptions` will not cause subsequent tests to fail.\n",
    "    \n",
    "\n",
    "3. **No jupyter shortcut commands, e.g.,** `! python script.py 10` or `%%timeit` **may be included in the final submission**: they will cause subsequent tests to fail.\n",
    "\n",
    "    - Comment out jupyter shortcut commands, e.g., `# ! python script.py 10` or `# %%timeit` in submitted notebooks.\n",
    "    \n",
    "\n",
    "4. Specific code solutions submitted for these assignments must be created either individually or in the context of a paired effort.\n",
    "  \n",
    "  - Students may work individually.  \n",
    "    - Students choosing to work individually must work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity)  of \"honesty, trust, fairness, respect, responsibility and courage.\"\n",
    "  - Students may self-select pairs for each assignment.\n",
    "    - Paired students work together and may share work without restriction within their pair; but, must otherwise work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity) noted above.\n",
    "    - Paired students each separately submit their (common) work, including (agreeing) contribution of work statements for each problem.\n",
    "    \n",
    "    *Please seek homework partners in person or on the course discussion board on Quercus. Groups of three or more are not allowed; however, students are welcome to amicably seek new partners for each new assignment.* \n",
    "\n",
    "  ***Getting and sharing \"hints\" from other classmates is allowed; but, the eventual code creation work and submission must be your own individual or paired creation.***\n",
    "\n",
    "\n",
    "5. The homework is open book, open notes, open internet, etc.\n",
    "\n",
    "6. You may use any functions available from all library imports below; otherwise, you are expected to create your own Python functionality based on the Python stdlib (standard libary).\n",
    "\n",
    "    ***Importing any libraries besides those specified below, those specifically suggested by a problem prompt, or the [standard python modules](https://docs.python.org/3/py-modindex.html) will cause a runtime error which will result in a loss of points.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e256d28",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# You may use any functions available from the following library imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as invlogit\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48de6e",
   "metadata": {},
   "source": [
    "# Problem 0 (required)\n",
    "\n",
    "Are you working with a partner to complete this assignment?  \n",
    "- If not, assign  the value of `None` into the variable `Partner`.\n",
    "- If so, assign the name of the person you worked with into the variable `Partner`.\n",
    "    - Format the name as `\"<First Name> <Last Name>\"` as a `str` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78081a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required\n",
    "Partner = #None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482b3e6",
   "metadata": {},
   "source": [
    "What was your contribution in completing the code for this assignments problems? Assign one of the following into each of the `Problem_X` variables below.\n",
    "\n",
    "- `\"I worked alone\"`\n",
    "- `\"I contributed more than my partner\"`\n",
    "- `\"My partner and I contributed equally\"`\n",
    "- `\"I contributed less than my partner\"`\n",
    "- `\"I did not contribute\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589b6a8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required\n",
    "Problem_1 = #\"I worked alone\"\n",
    "Problem_2 = #\"I worked alone\"\n",
    "Problem_3 = #\"I worked alone\"\n",
    "Problem_4 = #\"I worked alone\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08adf26b",
   "metadata": {},
   "source": [
    "# Problem 1 (6 points)\n",
    "\n",
    "A. Define the function `newton_raphson_iteration(f, df, x0, t, method='default')` which returns the `t`$^{th}$ step in ***Newton's root-finding method*** for any univariate function `f` such as\n",
    "\n",
    "$$f(x) = \\frac{\\log x}{1+x}$$\n",
    "\n",
    "B. Add the option `method='aikens_accelerated'` to the `newton_raphson_iteration` function which applies [***Aitken's $\\Delta^2$ acceleration***](https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process) to ***Newton's method***.\n",
    "\n",
    "C. Define the function `fixed_point_iteration(f, x0, t, a, method='default')` which returns the `t`$^{th}$ step in the ***fixed-point iteration algorithm for root-finding*** with ***scaling*** $\\alpha=$`a` for any univariate function `f` such as the one in part A above.\n",
    "\n",
    "> For a satisfactory $\\alpha$ this is also called the ***method of parallel chords*** because, regardless of $t$, the slope of the line from point $(x_t, f'(x_t))$ to point $(x_{t+1}, 0)$ is always constant since (\"rise over run\")\n",
    ">\n",
    "> $$\\frac{0-f'(x_t)}{x_{t+1}-x_t} = -\\frac{1}{\\alpha}$$\n",
    ">\n",
    ">  which can be seen from the parallel slopes in, e.g., \n",
    ">  ```\n",
    "  import matplotlib.pyplot as plt\n",
    "  import numpy as np \n",
    "  dfdx = lambda x: -4*x**3\n",
    "  x, alpha = 2/3, 0.1\n",
    "  for i in range(4):\n",
    "      x_t = x+alpha*dfdx(x)\n",
    "      plt.plot([x,x_t]+[x,x_t],[dfdx(x),0]+[dfdx(x),dfdx(x_t)],'k')\n",
    "      x = x_t\n",
    "> ```\n",
    "\n",
    "D. Add the option `method='steffensens_method'` to the `fixed_point_iteration` function which adds ***Steffensen's method*** (i.e., ***Aitken's $\\Delta^2$ acceleration***) to the ***fixed-point iteration algorithm***.\n",
    "\n",
    "*This problem is inspired by Example 2.2 **A Simple Univariate Optimization, Continued** in Section 2.1.1 **Newton's Method** in Chapter 2 **Optimization and Solving Nonlinear Equations**, and Example 2.3 **A Simple Univeriate Optimization, Continued** in Section 2.1.4.1 **Scaling** in Chapter 2.1.4 **Fixed-Point Iteration** of the Givens and Hoeting **Computational Statistics** textbook (pages 26-30 and 32-34).*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e28ddd",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def newton_raphson_iteration(f, df, x0, t, method = 'default'):\n",
    "    '''\n",
    "    The function implements the Newton-Raphson method for finding root x* so `f(x*)=0`\n",
    "    \n",
    "    x0     : intial value\n",
    "    t      : number of iterations (resulting in x_t or x_(t-2) with acceleration)\n",
    "    df     : derivative of f\n",
    "    f      : (default) lambda x: log(x)/(1+x)\n",
    "    method : either 'aikens_accelerated' or 'default' (no acceleration)\n",
    "\n",
    "    returns final_x,f(final_x)\n",
    "    '''   \n",
    "\n",
    "    xt = np.zeros(t+1)\n",
    "    xt[0] = x0\n",
    "    for i in range(1,t+1):\n",
    "        pass #<complete>\n",
    "            \n",
    "    if method == 'aikens_accelerated':\n",
    "        pass #<complete>\n",
    "        \n",
    "    return xt[-1], f(xt[-1])\n",
    "\n",
    "def fixed_point_iteration(f, x0, t, a, method = 'default'):\n",
    "    '''\n",
    "    The function implements scaled Fixed-Point iteration finding root \n",
    "    `f(x*)=0` iff `x* = x* + af(x*)`\n",
    "    \n",
    "    x0     : intial value\n",
    "    t      : number of iterations (resulting in x_t or x_(t-2) with acceleration)\n",
    "    a      : scaling factor alpha\n",
    "    df     : derivative of f\n",
    "    f      : (default) lambda x: log(x)/(1+x)\n",
    "    method : either 'aikens_accelerated' or 'default' (no acceleration)    \n",
    "\n",
    "    returns final_x,f(final_x)\n",
    "    '''\n",
    "    \n",
    "    xt = np.zeros(t+1)\n",
    "    xt[0] = x0\n",
    "    for i in range(1,t+1):\n",
    "        pass #<complete>\n",
    "    \n",
    "    if method == 'steffensens_method':\n",
    "        pass #<complete>\n",
    "    \n",
    "    return xt[-1], f(xt[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecad38",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "- This ***Aitken's $\\Delta^2$ acceleration*** [calculation example](https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process#Example_calculations) might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# add as many such cells as you like to testing and running your functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207afbf",
   "metadata": {},
   "source": [
    "### Problem 1 Questions 1-4 (2 points)\n",
    "\n",
    "For questions following questions use `f, x0, a = lambda x: np.log(x)/(1+x), 0.1, -0.5`.\n",
    "\n",
    "For questions 1 and 2, use `method=\"default\"`.\n",
    "\n",
    "1. (0.5 points) What is the smallest input `t` for which `newton_raphson_iteration` `|f(x_t)|<1e-15`?\n",
    "2. (0.5 points) What is the smallest input `t` for which `fixed_point_iteration` `|f(x_t)|<1e-15`?\n",
    "\n",
    "For questions 3 and 4, use ***Aitken's $\\Delta^2$ acceleration***.\n",
    "\n",
    "3. (0.5 points) What is the smallest input `t` for which `newton_raphson_iteration` `|f(x_t)|<1e-15`?\n",
    "4. (0.5 points) What is the smallest input `t` for which `fixed_point_iteration` `|f(x_t)|<1e-15`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033f684",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 2 points (0.5 each)\n",
    "p1q1 = # an integer, e.g., 10\n",
    "p1q2 = # an integer, e.g., 10\n",
    "p1q3 = # an integer, e.g., 10\n",
    "p1q4 = # an integer, e.g., 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b56426",
   "metadata": {},
   "source": [
    "### Problem 1 Questions 5-8 (2 points)\n",
    "\n",
    "For the following questions, choose one of \n",
    "\n",
    "- \"newton_raphson_iteration - default\"\n",
    "- \"newton_raphson_iteration - aikens_accelerated\"\n",
    "- \"fixed_point_iteration - default\"\n",
    "- \"fixed_point_iteration - steffensens_method\"\n",
    "- \"neither newton_raphson_iteration nor fixed_point_iteration\"\n",
    "\n",
    "5. (0.5 points) Which method is preferable for `f, x0, a = lambda x: np.log(x)/(1+x), 2.0, -0.5`?\n",
    "6. (0.5 points) Which method is preferable for `f, x0, a = lambda x: np.log(x)/(1+x), 4.0, -0.5`?\n",
    "7. (0.5 points) Which method is preferable for `f, x0, a = lambda x: np.log(x)/(1+x), 4.0, 0.5`?\n",
    "8. (0.5 points) Which method is preferable for `f, x0, a = lambda x: np.log(x)/(1+x), 0.5, 0.5`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06221f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 2 points (0.5 each)\n",
    "p1q5 = \"\"# a string \"<option chosen from above>\"\n",
    "p1q6 = \"\"# a string \"<option chosen from above>\"\n",
    "p1q7 = \"\"# a string \"<option chosen from above>\"\n",
    "p1q8 = \"\"# a string \"<option chosen from above>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9044e",
   "metadata": {},
   "source": [
    "### Problem 1 Questions 9-12 (2 points)\n",
    "\n",
    "Your functions will be evaluated for a different inputs `f` and `df` (or `a`).\n",
    "- You do not need to make any variable assignments: your function will be called based on the parameterization specified in the problem prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74684ec5",
   "metadata": {},
   "source": [
    "# Problem 2 (6 points)\n",
    "\n",
    "Define the function `logistic_regression_IRLS(X, y, beta0, t)` which returns $\\beta^{(t)}$ of a ***iteratively reweighted least squares*** (IRLS) fit \n",
    "\n",
    "1. $\\tilde y^{(t)} = \\underline{X\\beta^{(t)}}+ \\overset{(t)}{W}{}^{-1}(y- \\overset{(t)}{E[y]})$\n",
    "      - $E[y_i] = \\frac{1}{1+\\exp(-z_i\\beta)}$\n",
    "2. $\\beta^{(t+1)} = \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T\\overset{(t)}{W} \\tilde y^{(t)} \\quad \\underset{\\text{solved as}}{\\overset{\\text{efficiently}}{\\Longrightarrow}} \\quad \\left(X^T \\overset{(t)}{W}X\\right)\\beta^{(t+1)} = X^T\\overset{(t)}{W} \\tilde y^{(t)}$\n",
    "\n",
    "    - $W_{ij} = 0 \\text{ for } i\\not=j \\text{ and } W_{ii} = E[y_i] \\left(1-E[y_i] \\right)$\n",
    "\n",
    "of the logistic regression model\n",
    "   \n",
    "$\\quad\\quad \\Longrightarrow \\quad\\quad \\displaystyle \\Pr(y_i=1) = \\frac{1}{1+\\exp(-z_i\\beta)}$\n",
    "\n",
    "However\n",
    "\n",
    "1. rather than using the IRLS form exactly, instead use the standard form of ***Newton's Method*** where the $\\beta^{(t+1)}$ update above is instead reformulated as $\\beta^{(t+1)} = \\underline{\\beta^{(t)}} + \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T\\overset{(t)}{W} (\\tilde y^{(t)}-\\underline{X\\beta^{(t)}})$\n",
    "2. and ***do not*** actually do any matrix inversion: just use `np.linalg.solve`!\n",
    "\n",
    "\n",
    "*This problem is inspired by Subsection 2.2.1.1 **Iteratively Reweighted Least Squares** of Section 2.2.1 **Newton's Method and Fisher Scoring** in Chapter 2.2 **Multivariate Problems** of the Givens and Hoeting **Computational Statistics** textbook (pages 34-38).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743bd41b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit as invlogit\n",
    "def logistic_regression_IRLS(X, y, beta0, t):\n",
    "\n",
    "    beta_t = np.zeros((t,X.shape[1]))\n",
    "    beta_t[0] = beta0\n",
    "    \n",
    "    for i in range(1,t):\n",
    "        pass #<complete>\n",
    "        \n",
    "    return beta_t[-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f3ec8",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "- The `np.diag` function should be helpful.\n",
    "- Your algorithm can be compared against\n",
    "\n",
    "    - `statsmodels`\n",
    "    \n",
    "        ```\n",
    "        # https://www.geeksforgeeks.org/logistic-regression-using-statsmodels/\n",
    "        import statsmodels.api as sm\n",
    "        log_reg = sm.Logit(y[:,np.newaxis], X).fit()\n",
    "        log_reg.summary()\n",
    "        ```\n",
    "\n",
    "    - `scikit-learn`\n",
    "    \n",
    "        ```\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "        # https://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        logreg = LogisticRegression(penalty='none', fit_intercept=False)\n",
    "        logreg.fit(X, y)\n",
    "        logreg.coef_\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d036052",
   "metadata": {},
   "source": [
    "## Problem 2 Questions 1-3 (6 points)\n",
    "\n",
    "Your function will be tested against\n",
    "\n",
    "```\n",
    "from scipy.special import expit as invlogit\n",
    "np.random.seed(seed)\n",
    "X = np.random.normal(mu, sd, (n,p))\n",
    "X[:,0] = 1\n",
    "beta = np.random.normal(beta_mean, beta_sd, p)\n",
    "y = (np.random.uniform(size=n)<invlogit(X.dot(beta))).astype(int)\n",
    "```\n",
    "\n",
    "- You do not need to make any variable assignments: your function will be called based on the parameterization specified in the problem prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecad32d",
   "metadata": {},
   "source": [
    "# Problem 3 (8 points)\n",
    "\n",
    "Complete the function `nonlinear_gauss_seidel(f, x0, x_constraint, N=100, a=0.1, eps=0.1, K=20)` for use with the [Eggholder function](https://www.sfu.ca/~ssurjano/egg.html)\n",
    "\n",
    "$$-(x_2 + 47) \\sin\\left(\\sqrt{\\left|x_2+\\frac{x_1}{2} +47\\right|}\\right) - x_1\\sin\\left(\\sqrt{\\left| x_1 - (x_2+47)\\right|} \\right)$$\n",
    "\n",
    "*This problem draws upon the outstanding materials created by [Sonja Surjanovic and Derek Bingham](https://www.sfu.ca/~ssurjano/index.html) of the [Department of Statistics and Actuarial Science at Simon Fraser University](https://www.sfu.ca/stat-actsci.html); specifically, their [optimization resources](https://www.sfu.ca/~ssurjano/optimization.html) which includes an extensive collection of multimodal functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# https://www.tensorflow.org/guide/function#basics\n",
    "tf_Variable = tf.TensorSpec(shape=[], dtype=tf.float32)\n",
    "@tf.function(input_signature=(tf_Variable, tf_Variable, ))\n",
    "def eggholder(x1,x2):\n",
    "    y = -(x2+47)*tf.math.sin(tf.sqrt(tf.math.abs(x2+x1/2+47)))\n",
    "    return y - x1*tf.math.sin(tf.sqrt(tf.math.abs(x1-(x2+47))))\n",
    "\n",
    "def nonlinear_gauss_seidel(f, x0, x_constraint, N=20, a=0.1, eps=0.1, K=100):\n",
    "    \n",
    "    '''\n",
    "    Nonlinear Gauss-Seidel using Univariate Gradient Descent with TensorFlow\n",
    "    \n",
    "    f   : @tf.function(input_signature=(tf_Variable, tf_Variable, ))\n",
    "    x0  : (float,float) initialization \n",
    "    x_constraint : [[min_x1,max_x1],[min_x2,max_x2]) \n",
    "                   xi_t exceeding bounds is reassinged exceeded bound endpoint                   \n",
    "    N   : (default 100) number of Gauss-Seidel cycles\n",
    "    a   : (default 0.1) gradient descent step size factor\n",
    "    eps : (default 0.1) stopping criterion `|tape.gradient(y, x2)|<eps`\n",
    "    K   : (default 100) stopping criterion maximum number of gradient descent steps\n",
    "    \n",
    "    returns x1_N.numpy(),x2_N.numpy(),f(x1_N,x2_N).numpy()\n",
    "            where `_N` indicates completion of Nonlinear Gauss-Seidel cycles\n",
    "    '''\n",
    "    \n",
    "    x1 = tf.Variable(x0[0])\n",
    "    x2 = tf.Variable(x0[1])\n",
    "    \n",
    "    # <complete>\n",
    "                    \n",
    "    return x1.numpy(),x2.numpy(),f(x1,x2).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cbc24f",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "- Early stopping conditions can be enforced with `break`\n",
    "```\n",
    "for i in range(10):\n",
    "    if i==5:\n",
    "        break\n",
    "print(i)\n",
    "```\n",
    "\n",
    "- Numpy floating point type can be set with `dtype=np.float32`\n",
    "\n",
    "- TensorFlow operations require specific function calls, e.g., `y - x1*tf.math.sin(tf.math.abs(x1-(x2+47)))`\n",
    "\n",
    "- Parital derivatives in TensorFlow are calculated as\n",
    "```\n",
    "x1 = tf.Variable(x1_0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x1,x2)\n",
    "    dy_dx1 = tape.gradient(y, x1) # the derivative of (tf variable) y \n",
    "                                  # with respect to (tf variable) x1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc086098",
   "metadata": {},
   "source": [
    "## Problem 3 Questions 1-3 (6 points)\n",
    "\n",
    "Local minima will be found with you function for various initializations and parameter settings.\n",
    "\n",
    "- You do not need to assign any variables: your function will be called based on the parameterization specified in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a66e2",
   "metadata": {},
   "source": [
    "### Problem 3 Question 4 (2 points)\n",
    "\n",
    "What is the location of the minimum value of the ***Eggholder function*** subject to the constraint $x_1, x_2 \\in [-500,500]$ and what is that minimum value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628e0b1",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "p3q4 = (x1,x2,y) # tuple of floating point values with 3 decimal digits of precision after 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb0e20",
   "metadata": {},
   "source": [
    "# Problem 4 (7 points)\n",
    "\n",
    "Complete the function `newtons_method(f, x0, K=10, eps=1e-7)` for use with the $d$-variate [Schwefel function](https://www.sfu.ca/~ssurjano/schwef.html)\n",
    "\n",
    "$$418.9829d - \\sum_{i=1}^d x_i\\sin\\left(\\sqrt{|x_i|}\\right)$$\n",
    "\n",
    "\n",
    "  ```\n",
    "  import numpy as np\n",
    "  import tensorflow as tf\n",
    "  # https://www.tensorflow.org/guide/function#basics\n",
    "  # https://www.tensorflow.org/guide/function#controlling_retracing\n",
    "  @tf.function(input_signature=(tf.TensorSpec(shape=[2], dtype=tf.float64), ))\n",
    "  def f(x):\n",
    "      y = tf.math.reduce_sum(-x*tf.math.sin(tf.math.sqrt(tf.math.abs(x))))\n",
    "      return 418.9829*x.shape[0] + y\n",
    "  \n",
    "  limit = 200\n",
    "  K = 5 # improve stopping rule for number of Newton's method steps K?\n",
    "  grid = np.meshgrid(np.linspace(1,1,1), np.linspace(1,1,1))\n",
    "  for i,j in np.stack([ij.flatten() for ij in grid], axis=1):\n",
    "      x_k = tf.Variable([i,j])\n",
    "      for k in range(K):\n",
    "          with tf.GradientTape() as t2:\n",
    "              with tf.GradientTape() as t1:\n",
    "                  y = f(x_k)\n",
    "              # Compute the gradient inside the outer `t2` context manager\n",
    "              # which means the gradient computation is differentiable as well.\n",
    "              dy_dx = t1.gradient(y, x_k)\n",
    "          # https://www.tensorflow.org/guide/advanced_autodiff\n",
    "          d2y_dx2 = t2.jacobian(dy_dx, x_k)\n",
    "  \n",
    "          # enforce symmetry and ensure nonzero on the diagonal\n",
    "          d2y_dx2 = (tf.transpose(d2y_dx2)+d2y_dx2)/2 \n",
    "          d2y_dx2 = tf.where(tf.math.is_nan(d2y_dx2), tf.zeros_like(d2y_dx2), d2y_dx2)\n",
    "          d2y_dx2 = d2y_dx2 + tf.eye(x_k.shape[0], dtype=tf.float64)*1e-3 \n",
    "  \n",
    "          # \"Note: Don't actually invert the matrix.\"\n",
    "          # X(k+1) = X(k) - (∇²f(X(k)))^-1 @ ∇f(X(k))\n",
    "          # (∇²f(X(k)))[X(k)-X(k+1)] =  ∇f(X(k))\n",
    "          # [X(k)-X(k+1)] =  s\n",
    "          s = tf.linalg.solve(d2y_dx2, tf.reshape(dy_dx, [x_k.shape[0], 1]))\n",
    "          x_k = tf.Variable(tf.math.subtract(x_k, tf.reshape(s, x_k.shape[0])))\n",
    "  ```\n",
    "\n",
    "  *This problem draws upon the outstanding materials created by [Sonja Surjanovic and Derek Bingham](https://www.sfu.ca/~ssurjano/index.html) of the [Department of Statistics and Actuarial Science at Simon Fraser University](https://www.sfu.ca/stat-actsci.html); specifically, their [optimization resources](https://www.sfu.ca/~ssurjano/optimization.html) which includes an extensive collection of multimodal functions.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aaa4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3\n",
    "@tf.function(input_signature=(tf.TensorSpec(shape=[d], dtype=tf.float32), ))\n",
    "def schwefel(x):\n",
    "    y = tf.math.reduce_sum(x*tf.math.sin(tf.math.sqrt(tf.math.abs(x))))\n",
    "    return 418.9829*x.shape[0] - y\n",
    "\n",
    "def newtons_method(f, x0, K=10, eps=1e-7):\n",
    "    \n",
    "    '''\n",
    "    Newton's Method with TensorFlow\n",
    "    \n",
    "    f   : @tf.function(input_signature=(tf.TensorSpec(shape=[d], dtype=tf.float32), ))\n",
    "    x0  : [x0_0, x0_1, ..., x0_(d-1) initialization \n",
    "    K   : (default 10) number of Newton Method steps\n",
    "    eps : (default 0.01) stopping criterion `||x_k - x_(k-1)||_2<eps`\n",
    "    \n",
    "    returns x_k.numpy().tolist()+[f(x_k).numpy()]\n",
    "            where `_K` indicates the stopping criteria has been met\n",
    "    '''\n",
    "\n",
    "    x_k = tf.Variable(x0)\n",
    "    \n",
    "    # <complete>\n",
    "    \n",
    "    return x_k.numpy(),f(x_k).numpy()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ae3bb",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "- Examples of how to use TensorFlow to compute higher order partial derivatives are given here: https://www.tensorflow.org/guide/advanced_autodiff\n",
    "- You may ignore warning messages regarding \"triggered tf.function retracing\":\n",
    "    - these indicate that the same function is being repeatedly placed into the automatic differention graph, which happens intentionally in ***Newton's method*** since partial derivatives are being recalculated at different locations for each ***Newton step*** inside `for k in range(K)`.\n",
    "    - and the warnings may be silenced with \n",
    "    \n",
    "    ```\n",
    "    import logging, os\n",
    "    logging.disable(logging.WARNING)\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "    ```\n",
    "\n",
    "- If the computation of the ***Hessian*** $H$ is not ***symmetric***, $H + H^T$ will be ***symmetric***. \n",
    "- If the computation of the ***Hessian*** $H$ has `NaN`s or `0` diagonal elements then \"monkey patch\" them with \n",
    "    - `tf.where(tf.math.is_nan(H), tf.zeros_like(H), H)`\n",
    "    - `tf.eye(x_k.shape[0], dtype=tf.float32)*1e-7`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b8ba3",
   "metadata": {},
   "source": [
    "## Problem 4 Questions 1-2 (4 points)\n",
    "\n",
    "Local minima will be found with you function for various initializations and parameter settings.\n",
    "\n",
    "- You do not need to make any variable assignments: your function will be called based on the parameterization specified in the problem prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d017a4",
   "metadata": {},
   "source": [
    "## Problem 4 Questions 3-4 (2 points)\n",
    "\n",
    "The following questions will use the ***Schwefel function*** with $d=2$, i.e.,\n",
    "\n",
    "```\n",
    "d = 2\n",
    "@tf.function(input_signature=(tf.TensorSpec(shape=[d], dtype=tf.float32), ))\n",
    "def schwefel(x):\n",
    "    y = tf.math.reduce_sum(x*tf.math.sin(tf.math.sqrt(tf.math.abs(x))))\n",
    "    return 418.9829*x.shape[0] - y\n",
    "    \n",
    "tf_Variable = tf.TensorSpec(shape=[], dtype=tf.float32)\n",
    "@tf.function(input_signature=(tf_Variable, tf_Variable, ))\n",
    "def schwefel2(x1,x2):\n",
    "    x = tf.stack([x1,x2],axis=0)\n",
    "    y = tf.math.reduce_sum(x*tf.math.sin(tf.math.sqrt(tf.math.abs(x))))\n",
    "    return 418.9829*x.shape[0] - y\n",
    "   \n",
    "```\n",
    "\n",
    "and for each question you will \n",
    "- use the default values for `newtons_method` and `nonlinear_gauss_seidel`\n",
    "- \"turn off\" the `x_constraint` for `nonlinear_gauss_seidel` by setting it to be sufficiently large to not be violated\n",
    "- and choose one of\n",
    "    - \"newtons_method\"\n",
    "    - \"nonlinear_gauss_seidel\"\n",
    "    - \"either newtons_method or nonlinear_gauss_seidel\"\n",
    "    - \"neither newtons_method nor nonlinear_gauss_seidel\"\n",
    "\n",
    "5. (1 point) Which method is preferable for finding the global minimum near `x0=[450.,450.]`?\n",
    "6. (1 point) Which method is preferable for finding the global minimum near `x0=[480.,480.]`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d71b6",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p4q3 = \"\"# a string \"<option chosen from above>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad50c22",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p4q4 = \"\"# a string \"<option chosen from above>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4339f8",
   "metadata": {},
   "source": [
    "## Problem 4 Question 5 (1 point)\n",
    "\n",
    "What is the location of the minimum value of the $d=3$ ***Schwefel function*** subject to the constraint $x_1, x_2, x_3 \\in [-400,400]$ and $x_1 \\leq x_2 \\leq x_3$ and what is that minimum value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e73225",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1 point\n",
    "p4q5 = (x1,x2,x3,y) # tuple of floating point values with 2 decimal digits of precision after 0."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
